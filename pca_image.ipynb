{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f30a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks for Code from https://www.kaggle.com/mirzarahim/introduction-to-pca-image-compression-example\n",
    "# Introduction to PCA: Image Compression example\n",
    "\n",
    "# https://github.com/vivekrmk/Image-Compression-Principal-Component-Analysis-Pytorch/blob/main/Pytorch_PCA_journey.ipynb\n",
    "# https://github.com/Erikfather/PCA-python/blob/master/Face_Rec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309fdb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 2060\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Checking GPU Units\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import collections\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ee4477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-906f0233b0dd>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample['target'] = sample.label_group.map(tmp)\n"
     ]
    }
   ],
   "source": [
    "# DATA_PATH = '../../../Documents/shopee-product-matching/'\n",
    "DATA_PATH = 'shopee-product-matching/'\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.image import imread\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "train['image'] = DATA_PATH + 'train_images/' + train['image']\n",
    "test['image'] = DATA_PATH + 'test_images/' + test['image']\n",
    "\n",
    "n_train = 1200\n",
    "n_test = 3000\n",
    "n_valid = 300\n",
    "K = 500 # PCA, num of principal components\n",
    "\n",
    "sample = train.head(n_train)\n",
    "tmp = sample.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "sample['target'] = sample.label_group.map(tmp)\n",
    "#test   = train.loc[n_train+1:n_train+n_test]\n",
    "#test = test.reset_index(drop=True) # initialize indexing\n",
    "image_idx = sample['image']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c49c9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "test\n",
    "test_idx = test['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81402e65",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>shopee-product-matching/train_images/0000a6881...</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>shopee-product-matching/train_images/00039780d...</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>shopee-product-matching/train_images/000a190fd...</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>shopee-product-matching/train_images/00117e4fc...</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>shopee-product-matching/train_images/00136d1cf...</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>train_546357643</td>\n",
       "      <td>shopee-product-matching/train_images/5a4ff63df...</td>\n",
       "      <td>afe0f468ca9f9881</td>\n",
       "      <td>Skipping Jump Rope. Skipping Jump Digital Counter</td>\n",
       "      <td>47068657</td>\n",
       "      <td>[train_779610129, train_546357643]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>train_3553830288</td>\n",
       "      <td>shopee-product-matching/train_images/5a586ca7c...</td>\n",
       "      <td>e5969a49cc3633cc</td>\n",
       "      <td>[ TOPLES ] Kenko PC-3200JR Paper Clip - Warna ...</td>\n",
       "      <td>3403785568</td>\n",
       "      <td>[train_3553830288]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>train_2304804397</td>\n",
       "      <td>shopee-product-matching/train_images/5a5b56405...</td>\n",
       "      <td>ab618d3b91740d3e</td>\n",
       "      <td>Handgrip handfat grip karbon karet kitako ktc ...</td>\n",
       "      <td>474797793</td>\n",
       "      <td>[train_2304804397]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>train_2376848499</td>\n",
       "      <td>shopee-product-matching/train_images/5a5f2a56b...</td>\n",
       "      <td>c1b78fc375c70238</td>\n",
       "      <td>(Lochic) L-01 Sepatu LOL LED Anak Terbaru Impo...</td>\n",
       "      <td>4008024672</td>\n",
       "      <td>[train_2376848499, train_1934823359]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>train_1934823359</td>\n",
       "      <td>shopee-product-matching/train_images/5a5f2a56b...</td>\n",
       "      <td>c1b78fc375c70238</td>\n",
       "      <td>cc (L-01) 21-30 sepatu anak fashion LOL ala ko...</td>\n",
       "      <td>4008024672</td>\n",
       "      <td>[train_2376848499, train_1934823359]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                              image  \\\n",
       "0       train_129225211  shopee-product-matching/train_images/0000a6881...   \n",
       "1      train_3386243561  shopee-product-matching/train_images/00039780d...   \n",
       "2      train_2288590299  shopee-product-matching/train_images/000a190fd...   \n",
       "3      train_2406599165  shopee-product-matching/train_images/00117e4fc...   \n",
       "4      train_3369186413  shopee-product-matching/train_images/00136d1cf...   \n",
       "...                 ...                                                ...   \n",
       "11995   train_546357643  shopee-product-matching/train_images/5a4ff63df...   \n",
       "11996  train_3553830288  shopee-product-matching/train_images/5a586ca7c...   \n",
       "11997  train_2304804397  shopee-product-matching/train_images/5a5b56405...   \n",
       "11998  train_2376848499  shopee-product-matching/train_images/5a5f2a56b...   \n",
       "11999  train_1934823359  shopee-product-matching/train_images/5a5f2a56b...   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret   \n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "11995  afe0f468ca9f9881  Skipping Jump Rope. Skipping Jump Digital Counter   \n",
       "11996  e5969a49cc3633cc  [ TOPLES ] Kenko PC-3200JR Paper Clip - Warna ...   \n",
       "11997  ab618d3b91740d3e  Handgrip handfat grip karbon karet kitako ktc ...   \n",
       "11998  c1b78fc375c70238  (Lochic) L-01 Sepatu LOL LED Anak Terbaru Impo...   \n",
       "11999  c1b78fc375c70238  cc (L-01) 21-30 sepatu anak fashion LOL ala ko...   \n",
       "\n",
       "       label_group                                target  \n",
       "0        249114794                     [train_129225211]  \n",
       "1       2937985045                    [train_3386243561]  \n",
       "2       2395904891                    [train_2288590299]  \n",
       "3       4093212188  [train_2406599165, train_3342059966]  \n",
       "4       3648931069                    [train_3369186413]  \n",
       "...            ...                                   ...  \n",
       "11995     47068657    [train_779610129, train_546357643]  \n",
       "11996   3403785568                    [train_3553830288]  \n",
       "11997    474797793                    [train_2304804397]  \n",
       "11998   4008024672  [train_2376848499, train_1934823359]  \n",
       "11999   4008024672  [train_2376848499, train_1934823359]  \n",
       "\n",
       "[12000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efcc6366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>shopee-product-matching/test_images/0006c8e546...</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>shopee-product-matching/test_images/0007585c4d...</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>shopee-product-matching/test_images/0008377d36...</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_1122</td>\n",
       "      <td>shopee-product-matching/test_images/aaaaa.jpg</td>\n",
       "      <td>1a1a1a1a</td>\n",
       "      <td>phone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                              image  \\\n",
       "0  test_2255846744  shopee-product-matching/test_images/0006c8e546...   \n",
       "1  test_3588702337  shopee-product-matching/test_images/0007585c4d...   \n",
       "2  test_4015706929  shopee-product-matching/test_images/0008377d36...   \n",
       "3        test_1122      shopee-product-matching/test_images/aaaaa.jpg   \n",
       "\n",
       "        image_phash                                              title  \n",
       "0  ecc292392dc7687a  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  e9968f60d2699e2c  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2  ba81c17e3581cabe   READY Lemonilo Mie instant sehat kuah dan goreng  \n",
       "3          1a1a1a1a                                              phone  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b90fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1score(col):\n",
    "    def f1score(row):\n",
    "        n = len( np.intersect1d(row.target,row[col]) )\n",
    "        return 2*n / (len(row.target)+len(row[col]))\n",
    "    return f1score\n",
    "\n",
    "def getPrecision(col): # col = oof_cnn\n",
    "    def precision(row):\n",
    "        \n",
    "        a = np.in1d(row.target,row[col])\n",
    "        temp = collections.Counter(a)\n",
    "        correct = temp[True]/len(a)\n",
    "        \n",
    "        # if np.where(row.oof_cnn == row.target[0]) != []:\n",
    "        #     correct = 1\n",
    "        # else:  \n",
    "        #     correct = 0\n",
    "\n",
    "        return correct\n",
    "\n",
    "    return precision\n",
    "\n",
    "def getRecall(col):\n",
    "    def recall(row):\n",
    "        return 1/len(row[col])\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c31e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e706b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('L')\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab2ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Preparing train dataset, 2021. 5. 19\n",
    "train_img_list = image_idx\n",
    "\n",
    "mean = (0.0,)\n",
    "std = (1.0,)\n",
    "\n",
    "train_dataset = Img_Dataset(file_list = train_img_list,\n",
    "                            transform=ImageTransform(mean, std))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images = next(batch_iterator)\n",
    "\n",
    "print(images.size())\n",
    "\n",
    "# torch.Size[Batch Size, Channel, Width, Height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06788f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare test features 2021. 5. 19\n",
    "test_image_idx = test['image']\n",
    "test_img_list = test_image_idx\n",
    "\n",
    "test_dataset = Img_Dataset(file_list = test_img_list,\n",
    "                            transform=ImageTransform(mean, std))\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "batch_iterator = iter(test_dataloader)\n",
    "test_images = next(batch_iterator)\n",
    "\n",
    "len(batch_iterator)\n",
    "#print(test_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4d20b86",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1200 [00:00<?, ?it/s]<ipython-input-23-0ce96b9613de>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V = torch.tensor(V)\n",
      "  3%|██▏                                                                             | 32/1200 [00:03<02:05,  9.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0ce96b9613de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpca_lowrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#print('U shape : ', U.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_lowrank.py\u001b[0m in \u001b[0;36mpca_lowrank\u001b[1;34m(A, q, center, niter)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_svd_lowrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_lowrank.py\u001b[0m in \u001b[0;36m_svd_lowrank\u001b[1;34m(A, q, niter, M)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;31m# order to keep B shape minimal (the m < n case) or the V\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;31m# shape small (the n > q case)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_approximate_basis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mM_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[0mQ_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_lowrank.py\u001b[0m in \u001b[0;36mget_approximate_basis\u001b[1;34m(A, q, niter, M)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mM_H\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Making **train** features, 2021. 5. 19 \n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_feature = []\n",
    "train_feature = torch.tensor(train_feature)\n",
    "\n",
    "# transfer to DEVICE (GPU memory)\n",
    "train_feature = train_feature.to(DEVICE)\n",
    "\n",
    "a = 1\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        batch = batch.permute(0,2,3,1)[:,:,:,0]\n",
    "        idx, row, col = batch.shape\n",
    "        batch = batch.view([len(batch),-1])\n",
    "\n",
    "        U,S,V = torch.pca_lowrank(batch, q=len(batch), center=True, niter=3)\n",
    "        V = torch.tensor(V)\n",
    "        #print('U shape : ', U.shape)\n",
    "        #print('S shape : ', S.shape)\n",
    "        #print('V shape : ', V.shape)\n",
    "        train_feature = torch.cat([train_feature, V.T[:,:K]], dim = 0)\n",
    "        #print('train_feature shape: ', train_feature.shape)\n",
    "        \n",
    "        # For debugging (breaking)\n",
    "        #if a == 3:\n",
    "        #    break\n",
    "        #print('iter num : ', a)\n",
    "        a = a + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9436c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 1000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285f9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving **train** Features 2021. 5. 19\n",
    "train_feature = train_feature.data.cpu().numpy()\n",
    "np.savetxt('trained_feature.csv', train_feature, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74076b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading **train** Features 2021. 5. 19\n",
    "train_feature = np.loadtxt('trained_feature.csv', delimiter=\",\")\n",
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.to(DEVICE)\n",
    "\n",
    "# l2 norm to kill all the sim in 0-1   ** train_feature\n",
    "from sklearn.preprocessing import normalize\n",
    "train_feature = train_feature.data.cpu().numpy()\n",
    "train_feature = np.vstack(train_feature)\n",
    "train_feature = normalize(train_feature)\n",
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2d301c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degug(batch shape) :  torch.Size([4, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-e96b2c708ec7>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  V = torch.tensor(V)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Making **test** features, 2021. 5. 19 \n",
    "DEVICE = 'cuda'\n",
    "\n",
    "test_feature = []\n",
    "test_feature = torch.tensor(test_feature)\n",
    "\n",
    "# transfer to DEVICE (GPU memory)\n",
    "test_feature = test_feature.to(DEVICE)\n",
    "\n",
    "a = 1\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        #print('degug(batch shape) : ', batch.shape)\n",
    "        batch = batch.permute(0,2,3,1)[:,:,:,0]\n",
    "        idx, row, col = batch.shape\n",
    "        batch = batch.view([len(batch),-1])\n",
    "\n",
    "        U,S,V = torch.pca_lowrank(batch, q=len(batch), center=True, niter=3)\n",
    "        V = torch.tensor(V)\n",
    "        #print('U shape : ', U.shape)\n",
    "        #print('S shape : ', S.shape)\n",
    "        #print('V shape : ', V.shape)\n",
    "        \n",
    "        test_feature = torch.cat([test_feature, V.T[:,:K]], dim = 0)\n",
    "        #print('test_feature shape: ', test_feature.shape)\n",
    "        \n",
    "        # For debugging (breaking)\n",
    "        #if a == 3:\n",
    "        #    break\n",
    "        #print('iter num : ', a)\n",
    "        a = a + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c726f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving **test** Features 2021. 5. 19\n",
    "test_feature = test_feature.data.cpu().numpy()\n",
    "np.savetxt('test_feature.csv', test_feature, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b59bf9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading **test** Features 2021. 5. 19\n",
    "test_feature = np.loadtxt('test_feature.csv', delimiter=\",\")\n",
    "test_feature = torch.from_numpy(test_feature)\n",
    "test_feature = test_feature.to(DEVICE)\n",
    "\n",
    "# l2 norm to kill all the sim in 0-1    ** test_feature\n",
    "test_feature = test_feature.data.cpu().numpy()\n",
    "test_feature = np.vstack(test_feature)\n",
    "test_feature = normalize(test_feature)\n",
    "test_feature = torch.from_numpy(test_feature)\n",
    "test_feature = test_feature.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a001f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                               | 4/120 [00:00<00:06, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [00:05<00:00, 20.46it/s]\n",
      "<ipython-input-64-da2d257f8a70>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample['predicted_label'] = preds\n"
     ]
    }
   ],
   "source": [
    "# Checking train_feature with train_feature, 2021. 5. 19\n",
    "preds = []\n",
    "CHUNK = 100\n",
    "\n",
    "print('Finding similar images...')\n",
    "CTS = len(train_feature)//CHUNK\n",
    "if len(train_feature)%CHUNK != 0:\n",
    "    CTS += 1\n",
    "    \n",
    "for j in tqdm(range(CTS)):\n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b, len(train_feature))\n",
    "    #print('chunk', a, 'to', b)\n",
    "    \n",
    "    #distances = torch.cdist(train_feature, train_feature[a:b], p=2.0).T\n",
    "    distances = torch.matmul(train_feature, train_feature[a:b].T).T\n",
    "    distances = distances.data.cpu().numpy()\n",
    "    \n",
    "    #print(type(distances))\n",
    "    #print(distances.shape)\n",
    "    '''\n",
    "    for k in range(b-a):\n",
    "        IDX = np.argmin(distances[k][:])\n",
    "        o = sample.iloc[IDX].label_group\n",
    "        preds.append(o)\n",
    "    '''\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        #IDX = np.argmax(distances[k][:])\n",
    "        IDX = np.where(distances[k,]>0.98)[0][:]\n",
    "        #IDX = np.where(distances[k,]<0.1)[0][:]\n",
    "        #o = sample.iloc[IDX].label_group.values\n",
    "        o = sample.iloc[IDX].posting_id.values\n",
    "        preds.append(o)\n",
    "        #print(len(IDX))\n",
    "    \n",
    "sample['predicted_label'] = preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e691cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.84136441e+00, 1.69961600e+00, 7.03142078e-01, ...,\n",
       "        1.10858319e+00, 1.40383329e+00, 1.44742663e+00],\n",
       "       [8.14778854e-01, 1.07764920e+00, 1.77934641e+00, ...,\n",
       "        1.11576365e+00, 1.41985635e+00, 1.42193139e+00],\n",
       "       [1.20254826e+00, 1.23725036e+00, 1.66965049e+00, ...,\n",
       "        1.82141639e+00, 1.41708337e+00, 1.41142104e+00],\n",
       "       ...,\n",
       "       [1.53447938e+00, 1.55071492e+00, 1.22161698e+00, ...,\n",
       "        0.00000000e+00, 1.41496311e+00, 1.37124155e+00],\n",
       "       [1.42225938e+00, 1.40857082e+00, 1.40687874e+00, ...,\n",
       "        1.41496311e+00, 3.65002415e-08, 1.38613575e+00],\n",
       "       [1.42655339e+00, 1.45180531e+00, 1.40454195e+00, ...,\n",
       "        1.37124155e+00, 1.38613575e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "972dcc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-1e2c692ef894>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample['f1'] = sample.apply(getF1score('predicted_label'),axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for baseline =  0.5838659852896299\n",
      "precision =  0.6078847164137605\n",
      "recall =  0.8174100792345147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-1e2c692ef894>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample['Prec'] = sample.apply(getPrecision('predicted_label'),axis=1)\n",
      "<ipython-input-65-1e2c692ef894>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample['Rec'] = sample.apply(getRecall('predicted_label'),axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Scoring \n",
    "# 2021. 5. 20 edited\n",
    "\n",
    "sample['f1'] = sample.apply(getF1score('predicted_label'),axis=1)\n",
    "print('CV score for baseline = ', sample.f1.mean())\n",
    "sample['Prec'] = sample.apply(getPrecision('predicted_label'),axis=1)\n",
    "print('precision = ', sample.Prec.mean())\n",
    "sample['Rec'] = sample.apply(getRecall('predicted_label'),axis=1)\n",
    "print('recall = ', sample.Rec.mean())\n",
    "\n",
    "# Score history --------------------------------\n",
    "# 2021. 5. 20, K = 200, distances>0.95\n",
    "# F1 = 0.33\n",
    "# Pr = 0.45\n",
    "# Re = 0.56\n",
    "\n",
    "# 2021. 5. 20, K = 1000, samples = 12000, l2 distance < 0.1\n",
    "# CV score for baseline =  0.66716788743467\n",
    "# precision =  0.6060484848484848\n",
    "# recall =  0.9470901533062724\n",
    "\n",
    "# 2021. 5. 20, K = 1000, samples = 12000, cos-similarity > 0.98\n",
    "# CV score for baseline =  0.5838659852896299\n",
    "# precision =  0.6078847164137605\n",
    "# recall =  0.8174100792345147"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
