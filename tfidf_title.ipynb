{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks for Code from https://wikidocs.net/24603\n",
    "# Extracting features by using TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'shopee-product-matching/'\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "train = train.head(10000)\n",
    "text_data = train['title']\n",
    "tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "train['target'] = train.label_group.map(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1score(col):\n",
    "    def f1score(row):\n",
    "        n = len( np.intersect1d(row.target,row[col]) )\n",
    "        return 2*n / (len(row.target)+len(row[col]))\n",
    "    return f1score\n",
    "\n",
    "def getPrecision(col): # col = oof_cnn\n",
    "    def precision(row):\n",
    "        \n",
    "        a = np.in1d(row.target,row[col])\n",
    "        temp = collections.Counter(a)\n",
    "        correct = temp[True]/len(a)\n",
    "        \n",
    "        # if np.where(row.oof_cnn == row.target[0]) != []:\n",
    "        #     correct = 1\n",
    "        # else:  \n",
    "        #     correct = 0\n",
    "\n",
    "        return correct\n",
    "\n",
    "    return precision\n",
    "\n",
    "def getRecall(col):\n",
    "    def recall(row):\n",
    "        return 1/len(row[col])\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_svd(X, k, center=True):\n",
    "    n = X.size()[0]\n",
    "    print('n = :',n)\n",
    "    print('X shape :',X.shape)\n",
    "    ones = torch.ones(n).view([n,1])\n",
    "    h = ((1/n) * torch.mm(ones, ones.t())) if center  else torch.zeros(n*n).view([n,n])\n",
    "    H = torch.eye(n) - h\n",
    "    H = H.cuda()\n",
    "    print('H.double() : ', H.double().shape)\n",
    "    print('X.double() : ', X.double().shape)\n",
    "    X_center =  torch.mm(H.double(), X.double())\n",
    "    u, s, v = torch.svd(X_center)\n",
    "\n",
    "    print('v shape : ',v.shape)\n",
    "    components  = v[:k].t()\n",
    "    #explained_variance = torch.mul(s[:k], s[:k])/(n-1)\n",
    "    return components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>train_3058694204</td>\n",
       "      <td>4aede00854990e26f645d4c842b96754.jpg</td>\n",
       "      <td>ab1fd07094942bbd</td>\n",
       "      <td>Kaos Hoodie Anak TERLARIS/ Kaos Anak DISTRO Or...</td>\n",
       "      <td>793355432</td>\n",
       "      <td>[train_3058694204]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>train_1422100530</td>\n",
       "      <td>4aefbe697fe7691317349d75d8de799e.jpg</td>\n",
       "      <td>dbb1314bd278054f</td>\n",
       "      <td>Kotak Tempat Perhiasan dan Aksesoris Cincin Ge...</td>\n",
       "      <td>1035443562</td>\n",
       "      <td>[train_1422100530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>train_46290018</td>\n",
       "      <td>4af0235aee3fffabd7ba0867372d101b.jpg</td>\n",
       "      <td>bf0fc1e028e0571f</td>\n",
       "      <td>Makarizo hair energy shampoo 330ml</td>\n",
       "      <td>2944123046</td>\n",
       "      <td>[train_46290018]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>train_2643153468</td>\n",
       "      <td>4af24ae4093e6820f527483ac8e1bd8a.jpg</td>\n",
       "      <td>eaa9859689b796c1</td>\n",
       "      <td>Masker Wajah Partikel Rumput Laut 15g Untuk Pe...</td>\n",
       "      <td>114829279</td>\n",
       "      <td>[train_2070419749, train_2643153468]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>train_1518152019</td>\n",
       "      <td>4af293d98e0b89e05a831f014c518a01.jpg</td>\n",
       "      <td>9102f87edf31e0f0</td>\n",
       "      <td>headset superbass / U19 Macaron Color HIFI Hea...</td>\n",
       "      <td>2935813666</td>\n",
       "      <td>[train_2792135571, train_225931216, train_2429...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            posting_id                                 image  \\\n",
       "0      train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg   \n",
       "1     train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2     train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3     train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4     train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                ...                                   ...   \n",
       "9995  train_3058694204  4aede00854990e26f645d4c842b96754.jpg   \n",
       "9996  train_1422100530  4aefbe697fe7691317349d75d8de799e.jpg   \n",
       "9997    train_46290018  4af0235aee3fffabd7ba0867372d101b.jpg   \n",
       "9998  train_2643153468  4af24ae4093e6820f527483ac8e1bd8a.jpg   \n",
       "9999  train_1518152019  4af293d98e0b89e05a831f014c518a01.jpg   \n",
       "\n",
       "           image_phash                                              title  \\\n",
       "0     94974f937d4c2433                          Paper Bag Victoria Secret   \n",
       "1     af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2     b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3     8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4     a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                ...                                                ...   \n",
       "9995  ab1fd07094942bbd  Kaos Hoodie Anak TERLARIS/ Kaos Anak DISTRO Or...   \n",
       "9996  dbb1314bd278054f  Kotak Tempat Perhiasan dan Aksesoris Cincin Ge...   \n",
       "9997  bf0fc1e028e0571f                 Makarizo hair energy shampoo 330ml   \n",
       "9998  eaa9859689b796c1  Masker Wajah Partikel Rumput Laut 15g Untuk Pe...   \n",
       "9999  9102f87edf31e0f0  headset superbass / U19 Macaron Color HIFI Hea...   \n",
       "\n",
       "      label_group                                             target  \n",
       "0       249114794                                  [train_129225211]  \n",
       "1      2937985045                                 [train_3386243561]  \n",
       "2      2395904891                                 [train_2288590299]  \n",
       "3      4093212188                                 [train_2406599165]  \n",
       "4      3648931069                                 [train_3369186413]  \n",
       "...           ...                                                ...  \n",
       "9995    793355432                                 [train_3058694204]  \n",
       "9996   1035443562                                 [train_1422100530]  \n",
       "9997   2944123046                                   [train_46290018]  \n",
       "9998    114829279               [train_2070419749, train_2643153468]  \n",
       "9999   2935813666  [train_2792135571, train_225931216, train_2429...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00', 0),\n",
       " ('000', 1),\n",
       " ('0000', 2),\n",
       " ('000mah', 3),\n",
       " ('001', 4),\n",
       " ('002', 5),\n",
       " ('0026', 6),\n",
       " ('003', 7),\n",
       " ('0038', 8),\n",
       " ('003angka', 9),\n",
       " ('00405', 10),\n",
       " ('00406', 11),\n",
       " ('005', 12),\n",
       " ('006', 13),\n",
       " ('007', 14),\n",
       " ('0073', 15),\n",
       " ('008', 16),\n",
       " ('4kg', 993),\n",
       " ('4l', 994),\n",
       " ('4lbr', 995),\n",
       " ('4liter', 996),\n",
       " ('4m', 997),\n",
       " ('4mm', 998),\n",
       " ('4oz', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(text_data)\n",
    "tfidf_vectorizer.vocabulary_\n",
    "# sorted(tfidf_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape :  (10000, 14337)\n",
      "type :  <class 'numpy.ndarray'>\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Extracting features from text_data\n",
    "feature = tfidf_vectorizer.transform(text_data).toarray()\n",
    "#idx = np.where(feature != 0.)\n",
    "#print((feature))\n",
    "print('feature shape : ',feature.shape)\n",
    "print('type : ', type(feature))\n",
    "print(len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Components Analysis , temp, Latest one 2021. 5. 22\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "K = 50\n",
    "DEVICE = 'cuda'\n",
    "train_feature = []\n",
    "train_feature = torch.tensor(train_feature)\n",
    "train_feature = train_feature.to(DEVICE)\n",
    "\n",
    "batch = range(0, len(feature), 10)\n",
    "a = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    pca_feature = PCA(n_components = K)\n",
    "    principalComponents = pca_feature.fit_transform(feature)\n",
    "\n",
    "    principalComponents = torch.tensor(principalComponents)\n",
    "    principalComponents = principalComponents.to(DEVICE)\n",
    "\n",
    "    train_feature = principalComponents    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving **train** Features 2021. 5. 19\n",
    "train_feature = train_feature.data.cpu().numpy()\n",
    "np.savetxt('trained_text_feature.csv', train_feature, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading **train** Features 2021. 5. 19\n",
    "DEVICE = 'cuda'\n",
    "train_feature = np.loadtxt('trained_text_feature.csv', delimiter=\",\")\n",
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.to(DEVICE)\n",
    "\n",
    "# l2 norm to kill all the sim in 0-1   ** train_feature\n",
    "from sklearn.preprocessing import normalize\n",
    "train_feature = train_feature.data.cpu().numpy()\n",
    "train_feature = np.vstack(train_feature)\n",
    "train_feature = normalize(train_feature)\n",
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:00<00:20,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Checking train_text_feature with train_text_feature, 2021. 5. 21\n",
    "preds = []\n",
    "CHUNK = 100\n",
    "\n",
    "print('Finding similar images...')\n",
    "CTS = len(train_feature)//CHUNK\n",
    "if len(train_feature)%CHUNK != 0:\n",
    "    CTS += 1\n",
    "    \n",
    "for j in tqdm(range(CTS)):\n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b, len(train_feature))\n",
    "    \n",
    "    distances = torch.matmul(train_feature, train_feature[a:b].T).T\n",
    "    distances = distances.data.cpu().numpy()\n",
    "\n",
    "    for k in range(b-a):\n",
    "        #IDX = np.argmax(distances[k][:])\n",
    "        IDX = np.where(distances[k,]>0.95)[0][:]\n",
    "        #IDX = np.where(distances[k,]<0.1)[0][:]\n",
    "        #o = sample.iloc[IDX].label_group.values\n",
    "        o = train.iloc[IDX].posting_id.values\n",
    "        preds.append(o)\n",
    "        #print(len(IDX))\n",
    "    \n",
    "train['predicted'] = preds\n",
    "#del train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for baseline =  0.5514604356997606\n",
      "precision =  0.7496064232173055\n",
      "recall =  0.5379044077041456\n"
     ]
    }
   ],
   "source": [
    "train['f1'] = train.apply(getF1score('predicted'),axis=1)\n",
    "print('CV score for baseline = ', train.f1.mean())\n",
    "train['Prec'] = train.apply(getPrecision('predicted'),axis=1)\n",
    "print('precision = ', train.Prec.mean())\n",
    "train['Rec'] = train.apply(getRecall('predicted'),axis=1)\n",
    "print('recall = ', train.Rec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train_129225211', 'train_1220997311', 'train_1941131050',\n",
       "       'train_3243826013', 'train_2078576963', 'train_2344463199'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['predicted'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
